{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "558711d1",
      "metadata": {
        "id": "558711d1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "274b6bd2",
      "metadata": {
        "id": "274b6bd2"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- NeuralNetwork Class ---\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    Neural Network model with L hidden layers and a single output (y)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.num_layers = len(self.layer_sizes) - 1\n",
        "\n",
        "        self.initialize_network()\n",
        "\n",
        "    def initialize_network(self):\n",
        "        \"\"\"Initializes weights and biases for all layers.\"\"\"\n",
        "        for i in range(self.num_layers):\n",
        "            input_dim = self.layer_sizes[i]\n",
        "            output_dim = self.layer_sizes[i+1]\n",
        "\n",
        "            # Xavier initialization for parameters\n",
        "            scale_w = np.sqrt(2.0 / (input_dim + output_dim))\n",
        "            self.weights.append(np.random.normal(loc=0, scale=scale_w, size=(input_dim, output_dim)))\n",
        "            self.biases.append(np.random.normal(loc=0, scale=0.01, size=(1, output_dim)))\n",
        "\n",
        "        self.n_params = sum(w.size for w in self.weights) + sum(b.size for b in self.biases)\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"Softmax activation function.\"\"\"\n",
        "        exp_X = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
        "        return exp_X / np.sum(exp_X, axis=-1, keepdims=True)\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"Performs a forward pass and stores intermediate activations and Z-values.\"\"\"\n",
        "        self.activations = [X]\n",
        "        self.zs = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            current_input = self.activations[-1]\n",
        "            weights_l = self.weights[i]\n",
        "            biases_l = self.biases[i]\n",
        "\n",
        "            Z = np.dot(current_input, weights_l) + biases_l\n",
        "            self.zs.append(Z)\n",
        "\n",
        "            if i < self.num_layers - 1: # Hidden layers use sigmoid\n",
        "                A = self.sigmoid(Z)\n",
        "            else: # Output layer uses softmax\n",
        "                A = self.softmax(Z)\n",
        "            self.activations.append(A)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward_pass(self, X, Y):\n",
        "        \"\"\"Computes gradients and updates weights/biases using backpropagation.\"\"\"\n",
        "        output_activations = self.activations[-1]\n",
        "        delta = output_activations - Y # Delta for last layer\n",
        "        deltas = [delta]\n",
        "\n",
        "        # Propagate deltas backwards through hidden layers\n",
        "        for i in range(self.num_layers - 1, 0, -1):\n",
        "            W_l = self.weights[i]\n",
        "            Z_prev = self.zs[i-1]\n",
        "\n",
        "            delta = (deltas[-1] @ W_l.T) * (self.sigmoid(Z_prev) * (1 - self.sigmoid(Z_prev)))\n",
        "            deltas.append(delta)\n",
        "\n",
        "        deltas.reverse() # Reverse to match forward order (delta[0] for first layer, etc.)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(self.num_layers):\n",
        "            input_to_weights = self.activations[i]\n",
        "            weights_delta = np.dot(input_to_weights.T, deltas[i])\n",
        "            biases_delta = np.sum(deltas[i], axis=0, keepdims=True) # Sum delta over samples for biases\n",
        "\n",
        "            self.weights[i] -= self.learning_rate * weights_delta\n",
        "            self.biases[i] -= self.learning_rate * biases_delta\n",
        "\n",
        "    def encode(self):\n",
        "        \"\"\"Encodes all model parameters (weights and biases) into a single 1D vector.\"\"\"\n",
        "        all_params = []\n",
        "        for w in self.weights:\n",
        "            all_params.append(w.ravel())\n",
        "        for b in self.biases:\n",
        "            all_params.append(b.ravel())\n",
        "        theta = np.concatenate(all_params)\n",
        "        return theta\n",
        "\n",
        "    def decode(self, theta):\n",
        "        \"\"\"Decodes a 1D parameter vector into the model's weights and biases.\"\"\"\n",
        "        decoded_weights = []\n",
        "        decoded_biases = []\n",
        "        offset = 0\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            input_dim = self.layer_sizes[i]\n",
        "            output_dim = self.layer_sizes[i+1]\n",
        "\n",
        "            # Decode weights\n",
        "            weight_size = input_dim * output_dim\n",
        "            W_l = theta[offset : offset + weight_size].reshape(input_dim, output_dim)\n",
        "            decoded_weights.append(W_l)\n",
        "            offset += weight_size\n",
        "\n",
        "            # Decode biases\n",
        "            bias_size = output_dim\n",
        "            B_l = theta[offset : offset + bias_size].reshape(1, output_dim)\n",
        "            decoded_biases.append(B_l)\n",
        "            offset += bias_size\n",
        "\n",
        "        self.weights = decoded_weights\n",
        "        self.biases = decoded_biases\n",
        "\n",
        "    def evaluate_proposal(self, theta, X_data):\n",
        "        \"\"\"\n",
        "        Evaluates the network's output for given data using a specific parameter vector 'theta'.\n",
        "        \"\"\"\n",
        "        # Temporarily decode theta into local weights/biases for evaluation\n",
        "        temp_weights = []\n",
        "        temp_biases = []\n",
        "        offset = 0\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            input_dim = self.layer_sizes[i]\n",
        "            output_dim = self.layer_sizes[i+1]\n",
        "\n",
        "            weight_size = input_dim * output_dim\n",
        "            W_l = theta[offset : offset + weight_size].reshape(input_dim, output_dim)\n",
        "            temp_weights.append(W_l)\n",
        "            offset += weight_size\n",
        "\n",
        "            bias_size = output_dim\n",
        "            B_l = theta[offset : offset + bias_size].reshape(1, output_dim)\n",
        "            temp_biases.append(B_l)\n",
        "            offset += bias_size\n",
        "\n",
        "        # Perform forward pass using temp_weights and temp_biases\n",
        "        activations = [X_data]\n",
        "        for i in range(self.num_layers - 1): # Hidden layers\n",
        "            current_input = activations[-1]\n",
        "            Z = np.dot(current_input, temp_weights[i]) + temp_biases[i]\n",
        "            A = self.sigmoid(Z)\n",
        "            activations.append(A)\n",
        "\n",
        "        # Output layer\n",
        "        final_input = activations[-1]\n",
        "        Z_out = np.dot(final_input, temp_weights[-1]) + temp_biases[-1]\n",
        "        fx = self.softmax(Z_out)\n",
        "\n",
        "        return fx\n",
        "\n",
        "    def langevin_gradient(self, x_data, y_data, theta, depth):\n",
        "        \"\"\"\n",
        "        Computes a gradient-based proposal for MCMC.\n",
        "        Temporarily applies `theta` to the model, performs SGD steps, then encodes and returns.\n",
        "        \"\"\"\n",
        "        # Save current model state\n",
        "        original_weights = [w.copy() for w in self.weights]\n",
        "        original_biases = [b.copy() for b in self.biases]\n",
        "\n",
        "        # Apply the input theta to the model\n",
        "        self.decode(theta)\n",
        "\n",
        "        # Perform SGD steps\n",
        "        for _ in range(0, depth):\n",
        "            self.forward_pass(x_data)\n",
        "            self.backward_pass(x_data, y_data)\n",
        "\n",
        "        # Get the updated theta\n",
        "        theta_updated = self.encode()\n",
        "\n",
        "        # Restore original model state\n",
        "        self.weights = original_weights\n",
        "        self.biases = original_biases\n",
        "\n",
        "        return theta_updated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a0618e90",
      "metadata": {
        "id": "a0618e90"
      },
      "outputs": [],
      "source": [
        "# --- MCMC Class ---\n",
        "class MCMC:\n",
        "    def __init__(self, n_samples, n_burnin, x_data, y_data, x_test, y_test, layer_sizes):\n",
        "        self.n_samples = n_samples\n",
        "        self.n_burnin = n_burnin\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        self.step_theta = 0.025 # Step size for the MCMC proposal (also called  std dev)\n",
        "        self.sigma_squared = 25 # Variance for the Gaussian prior on parameters\n",
        "\n",
        "        self.model = NeuralNetwork(layer_sizes)\n",
        "        self.theta_size = self.model.n_params\n",
        "\n",
        "        self.use_langevin_gradients = True # whether to use langevin or not\n",
        "        self.sgd_depth = 1 # Number of SGD steps for Langevin gradient\n",
        "        self.l_prob = 0.5 # Probability of using Langevin proposal vs. simple random walk\n",
        "\n",
        "        # Storage for posterior samples and predictions\n",
        "        self.pos_theta = None\n",
        "        self.pred_y = None\n",
        "        self.sim_y = None\n",
        "        self.accuracy_data = None\n",
        "        self.test_pred_y = None\n",
        "        self.test_sim_y = None\n",
        "        self.test_accuracy_data = None\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(predictions, targets):\n",
        "        \"\"\"Calculates classification accuracy.\"\"\"\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        if targets.ndim > 1 and targets.shape[1] > 1: # as sometime people do not OHE the output\n",
        "            true_classes = np.argmax(targets, axis=1)\n",
        "        else: # Assuming  targets are already integer labels or flattened 1D array\n",
        "            true_classes = targets.flatten()\n",
        "\n",
        "        return accuracy_score(true_classes, predicted_classes) * 100\n",
        "\n",
        "    def likelihood_function(self, theta, test=False):\n",
        "        \"\"\"Calculates the multinomial log-likelihood (cross-entropy) and accuracy.\"\"\"\n",
        "        x_data_eval = self.x_test if test else self.x_data\n",
        "        y_data_eval = self.y_test if test else self.y_data\n",
        "\n",
        "        model_prediction = self.model.evaluate_proposal(theta, x_data_eval)\n",
        "        model_prediction = np.maximum(model_prediction, 1e-12) # Clipping  to prevent log(0) and stability\n",
        "\n",
        "        log_likelihood = np.sum(y_data_eval * np.log(model_prediction))\n",
        "        accuracy = self.accuracy(model_prediction, y_data_eval)\n",
        "        model_simulation = model_prediction # we can use it later for data generation\n",
        "\n",
        "        return log_likelihood, model_prediction, model_simulation, accuracy\n",
        "\n",
        "    def prior(self, sigma_squared, theta):\n",
        "        \"\"\"Calculates the log-prior probability for parameters (Gaussian prior).\"\"\"\n",
        "        part1 = -self.model.n_params / 2.0 * np.log(sigma_squared)\n",
        "        part2 = 1 / (2.0 * sigma_squared) * (np.sum(np.square(theta)))\n",
        "        log_prior = part1 - part2\n",
        "        return log_prior\n",
        "\n",
        "    def MCMC_sampler(self):\n",
        "        \"\"\"Runs the MCMC sampler to generate posterior samples.\"\"\"\n",
        "        output_dim = self.model.layer_sizes[-1]\n",
        "        pos_theta = np.zeros((self.n_samples, self.theta_size))\n",
        "        pred_y = np.zeros((self.n_samples, self.x_data.shape[0], output_dim))\n",
        "        test_pred_y = np.zeros((self.n_samples, self.x_test.shape[0], output_dim))\n",
        "        sim_y = np.zeros((self.n_samples, self.x_data.shape[0], output_dim))\n",
        "        test_sim_y = np.zeros((self.n_samples, self.x_test.shape[0], output_dim))\n",
        "        accuracy_data = np.zeros(self.n_samples)\n",
        "        test_accuracy_data = np.zeros(self.n_samples)\n",
        "\n",
        "        # Initialisation: starting point for the chain\n",
        "        theta = np.random.randn(self.theta_size) # Initial random parameters\n",
        "\n",
        "        # Calculate initial likelihood and prior\n",
        "        prior_val = self.prior(self.sigma_squared, theta)\n",
        "        (likelihood, initial_train_pred, _, accuracy_train) = self.likelihood_function(theta, test=False)\n",
        "        (test_likelihood, initial_test_pred, _, accuracy_test) = self.likelihood_function(theta, test=True)\n",
        "\n",
        "        pos_theta[0, :] = theta\n",
        "        pred_y[0, :, :] = initial_train_pred\n",
        "        sim_y[0, :, :] = initial_train_pred\n",
        "        accuracy_data[0] = accuracy_train\n",
        "        test_pred_y[0, :, :] = initial_test_pred\n",
        "        test_sim_y[0, :, :] = initial_test_pred\n",
        "        test_accuracy_data[0] = accuracy_test\n",
        "\n",
        "        n_accepted_samples = 0\n",
        "\n",
        "        print(f\"MCMC Chain started for {self.n_samples} samples...\")\n",
        "        print(f\"Initial Train Acc: {accuracy_train:.2f}%, Test Acc: {accuracy_test:.2f}%\")\n",
        "\n",
        "        for ii in range(1, self.n_samples):\n",
        "            theta_current = pos_theta[ii - 1, :]\n",
        "            prior_current = prior_val\n",
        "            likelihood_current = likelihood\n",
        "\n",
        "            lx = np.random.uniform(0, 1)\n",
        "            if lx < self.l_prob and self.use_langevin_gradients:\n",
        "                theta_gd = self.model.langevin_gradient(self.x_data, self.y_data, theta_current, self.sgd_depth)\n",
        "                theta_proposal = np.random.normal(theta_gd, self.step_theta, self.theta_size)\n",
        "            else:\n",
        "                theta_proposal = np.random.normal(theta_current, self.step_theta, self.theta_size)\n",
        "\n",
        "            prior_proposal = self.prior(self.sigma_squared, theta_proposal)\n",
        "            (likelihood_proposal, current_train_pred, _, accuracy_train_proposal) = self.likelihood_function(theta_proposal, test=False)\n",
        "            (test_likelihood_proposal, current_test_pred, _, accuracy_test_proposal) = self.likelihood_function(theta_proposal, test=True)\n",
        "\n",
        "            diff_likelihood = likelihood_proposal - likelihood_current\n",
        "            diff_prior = prior_proposal - prior_current\n",
        "\n",
        "            diff_prop = 0.0\n",
        "\n",
        "            if lx < self.l_prob and self.use_langevin_gradients:\n",
        "                theta_gd_reverse = self.model.langevin_gradient(self.x_data, self.y_data, theta_proposal, self.sgd_depth)\n",
        "\n",
        "                log_q_prop_given_curr = -0.5 * np.sum(np.square(theta_proposal - theta_gd)) / (self.step_theta**2)\n",
        "                log_q_curr_given_prop = -0.5 * np.sum(np.square(theta_current - theta_gd_reverse)) / (self.step_theta**2)\n",
        "\n",
        "                diff_prop = log_q_curr_given_prop - log_q_prop_given_curr\n",
        "\n",
        "            mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
        "\n",
        "            u = np.random.uniform(0, 1)\n",
        "\n",
        "            if u < mh_prob:\n",
        "                pos_theta[ii, :] = theta_proposal\n",
        "                likelihood = likelihood_proposal\n",
        "                prior_val = prior_proposal\n",
        "                pred_y[ii, :, :] = current_train_pred\n",
        "                sim_y[ii, :, :] = current_train_pred\n",
        "                accuracy_data[ii] = accuracy_train_proposal\n",
        "                test_pred_y[ii, :, :] = current_test_pred\n",
        "                test_sim_y[ii, :, :] = current_test_pred\n",
        "                test_accuracy_data[ii] = accuracy_test_proposal\n",
        "                n_accepted_samples += 1\n",
        "            else:\n",
        "                pos_theta[ii, :] = pos_theta[ii - 1, :]\n",
        "                pred_y[ii, :, :] = pred_y[ii - 1, :, :]\n",
        "                sim_y[ii, :, :] = sim_y[ii - 1, :, :]\n",
        "                accuracy_data[ii] = accuracy_data[ii - 1]\n",
        "                test_pred_y[ii, :, :] = test_pred_y[ii - 1, :, :]\n",
        "                test_sim_y[ii, :, :] = test_sim_y[ii - 1, :, :]\n",
        "                test_accuracy_data[ii] = test_accuracy_data[ii - 1]\n",
        "\n",
        "            if (ii + 1) % 500 == 0:\n",
        "                acceptance_rate = (n_accepted_samples / (ii + 1)) * 100\n",
        "                print(f\"Sample {ii+1}/{self.n_samples} | Accept Rate: {acceptance_rate:.2f}% | \"\n",
        "                      f\"Train Acc: {accuracy_data[ii]:.2f}% | Test Acc: {test_accuracy_data[ii]:.2f}%\")\n",
        "\n",
        "        print(\"MCMC sampling complete. Applying burn-in.\")\n",
        "\n",
        "        self.pos_theta = pos_theta[self.n_burnin:, :]\n",
        "        self.pred_y = pred_y[self.n_burnin:, :, :]\n",
        "        self.sim_y = sim_y[self.n_burnin:, :, :]\n",
        "        self.accuracy_data = accuracy_data[self.n_burnin:]\n",
        "        self.test_pred_y = test_pred_y[self.n_burnin:, :, :]\n",
        "        self.test_sim_y = test_sim_y[self.n_burnin:, :, :]\n",
        "        self.test_accuracy_data = test_accuracy_data[self.n_burnin:]\n",
        "\n",
        "        results_df = pd.DataFrame(self.pos_theta, columns=[f\"theta_{i}\" for i in range(self.theta_size)])\n",
        "        return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "34abd768",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34abd768",
        "outputId": "d33e527c-30e2-4971-f1b2-7ec34a816981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network Architecture: [4, 10, 5, 3]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Data Preparation and Execution ---\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encode target labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define network architecture\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "hidden_layer_nodes = [10, 5]\n",
        "layer_sizes = [input_dim] + hidden_layer_nodes + [output_dim]\n",
        "\n",
        "print(f\"Network Architecture: {layer_sizes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "625eb250",
      "metadata": {
        "id": "625eb250"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2c44c444",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c44c444",
        "outputId": "5603d6ed-285a-45d9-9c12-550eb8cd99b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting MCMC sampling...\n",
            "MCMC Chain started for 5000 samples...\n",
            "Initial Train Acc: 33.33%, Test Acc: 33.33%\n",
            "Sample 500/5000 | Accept Rate: 45.80% | Train Acc: 98.33% | Test Acc: 96.67%\n",
            "Sample 1000/5000 | Accept Rate: 47.60% | Train Acc: 98.33% | Test Acc: 100.00%\n",
            "Sample 1500/5000 | Accept Rate: 46.53% | Train Acc: 98.33% | Test Acc: 100.00%\n",
            "Sample 2000/5000 | Accept Rate: 45.90% | Train Acc: 98.33% | Test Acc: 96.67%\n",
            "Sample 2500/5000 | Accept Rate: 46.88% | Train Acc: 99.17% | Test Acc: 100.00%\n",
            "Sample 3000/5000 | Accept Rate: 47.27% | Train Acc: 98.33% | Test Acc: 96.67%\n",
            "Sample 3500/5000 | Accept Rate: 47.40% | Train Acc: 99.17% | Test Acc: 96.67%\n",
            "Sample 4000/5000 | Accept Rate: 46.75% | Train Acc: 100.00% | Test Acc: 93.33%\n",
            "Sample 4500/5000 | Accept Rate: 47.02% | Train Acc: 100.00% | Test Acc: 96.67%\n",
            "Sample 5000/5000 | Accept Rate: 47.44% | Train Acc: 100.00% | Test Acc: 96.67%\n",
            "MCMC sampling complete. Applying burn-in.\n",
            "MCMC sampling finished.\n",
            "\n",
            "Shape of posterior samples (excluding the burn-in period ): (4000, 123)\n",
            "First 5 posterior samples:\n",
            "     theta_0   theta_1   theta_2   theta_3   theta_4   theta_5   theta_6  \\\n",
            "0 -1.048511  0.941178  2.513171 -0.248311 -0.575973  0.514084 -0.124257   \n",
            "1 -1.024232  0.926082  2.475127 -0.250200 -0.574626  0.538089 -0.126155   \n",
            "2 -1.023922  0.922791  2.475222 -0.248283 -0.574377  0.538083 -0.121941   \n",
            "3 -1.023616  0.919617  2.475322 -0.246457 -0.574098  0.538081 -0.117920   \n",
            "4 -1.023321  0.916624  2.475430 -0.244814 -0.573719  0.538089 -0.114215   \n",
            "\n",
            "    theta_7   theta_8   theta_9  ...  theta_113  theta_114  theta_115  \\\n",
            "0 -1.683646  0.264664  0.608930  ...  -4.596727  -1.515611  -3.715611   \n",
            "1 -1.656441  0.263904  0.608559  ...  -4.599031  -1.534061  -3.657108   \n",
            "2 -1.656528  0.262121  0.605309  ...  -4.600774  -1.534408  -3.657837   \n",
            "3 -1.656614  0.260410  0.602164  ...  -4.602414  -1.534751  -3.658679   \n",
            "4 -1.656701  0.258803  0.599197  ...  -4.604032  -1.535091  -3.659536   \n",
            "\n",
            "   theta_116  theta_117  theta_118  theta_119  theta_120  theta_121  theta_122  \n",
            "0   2.911782  -2.583895   1.419701   1.225506  -0.842070   0.285430   1.237603  \n",
            "1   2.922902  -2.565254   1.394099   1.244214  -0.846706   0.257500   1.251750  \n",
            "2   2.923978  -2.566378   1.395224   1.244213  -0.846807   0.257799   1.251551  \n",
            "3   2.925163  -2.567495   1.396162   1.244392  -0.846900   0.257904   1.251539  \n",
            "4   2.926360  -2.568606   1.397072   1.244593  -0.846987   0.257978   1.251551  \n",
            "\n",
            "[5 rows x 123 columns]\n"
          ]
        }
      ],
      "source": [
        "# MCMC parameters\n",
        "n_samples = 5000  # Number of MCMC samples\n",
        "n_burnin = 1000   # Number of burn-in samples\n",
        "\n",
        "# Initialize and run MCMC sampler\n",
        "print(\"Starting MCMC sampling...\")\n",
        "bnn_mcmc = MCMC(n_samples, n_burnin, X_train, y_train, X_test, y_test, layer_sizes)\n",
        "posterior_samples_df = bnn_mcmc.MCMC_sampler()\n",
        "print(\"MCMC sampling finished.\")\n",
        "\n",
        "print(\"\\nShape of posterior samples (excluding the burn-in period ):\", posterior_samples_df.shape)\n",
        "print(\"First 5 posterior samples:\\n\", posterior_samples_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2fab4cf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fab4cf1",
        "outputId": "c8bd02cf-631d-4ac3-a322-8f2144955cdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Final Performance Evaluation\n",
            "\n",
            "Final Train Accuracy (from posterior predictive mean): 99.17%\n",
            "Final Test Accuracy (from posterior predictive mean): 96.67%\n",
            "\n",
            "Test Classification Report (from posterior predictive mean):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.90      1.00      0.95         9\n",
            "   virginica       1.00      0.91      0.95        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Average Train Accuracy (across accepted samples): 99.10%\n",
            "Average Test Accuracy (across accepted samples): 96.63%\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluate Performance ---\n",
        "print(\"\\n Final Performance Evaluation\")\n",
        "\n",
        "# Convert one-hot encoded true labels to integer labels for accuracy_score\n",
        "true_train_labels = np.argmax(y_train, axis=1)\n",
        "true_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# 1. Using the mean of predictions from all accepted posterior samples (Posterior Predictive Mean)\n",
        "# This averages the probabilities across all samples, we will take argmax\n",
        "mean_train_predictions_prob = np.mean(bnn_mcmc.pred_y, axis=0)\n",
        "mean_test_predictions_prob = np.mean(bnn_mcmc.test_pred_y, axis=0)\n",
        "\n",
        "train_predicted_labels_mean_pred = np.argmax(mean_train_predictions_prob, axis=1)\n",
        "test_predicted_labels_mean_pred = np.argmax(mean_test_predictions_prob, axis=1)\n",
        "\n",
        "final_train_accuracy_mean_pred = accuracy_score(true_train_labels, train_predicted_labels_mean_pred) * 100\n",
        "final_test_accuracy_mean_pred = accuracy_score(true_test_labels, test_predicted_labels_mean_pred) * 100\n",
        "\n",
        "print(f\"\\nFinal Train Accuracy (from posterior predictive mean): {final_train_accuracy_mean_pred:.2f}%\")\n",
        "print(f\"Final Test Accuracy (from posterior predictive mean): {final_test_accuracy_mean_pred:.2f}%\")\n",
        "\n",
        "print(\"\\nTest Classification Report (from posterior predictive mean):\")\n",
        "print(classification_report(true_test_labels, test_predicted_labels_mean_pred, target_names=iris.target_names))\n",
        "\n",
        "# 2. Average of individual sample accuracies\n",
        "avg_sampled_train_accuracy = np.mean(bnn_mcmc.accuracy_data)\n",
        "avg_sampled_test_accuracy = np.mean(bnn_mcmc.test_accuracy_data)\n",
        "print(f\"\\nAverage Train Accuracy (across accepted samples): {avg_sampled_train_accuracy:.2f}%\")\n",
        "print(f\"Average Test Accuracy (across accepted samples): {avg_sampled_test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa196fb2",
      "metadata": {
        "id": "fa196fb2"
      },
      "source": [
        "## SKLEARN RESULTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43b6e8dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b6e8dc",
        "outputId": "5b617d46-e3cb-4bc7-9f24-bd2d1e0f5236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training Frequentist Neural Network ---\n",
            "\n",
            "--- Evaluating Frequentist Neural Network Performance ---\n",
            "Train Accuracy: 98.33%\n",
            "Test Accuracy: 100.00%\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "--- Frequentist Neural Network Performance Evaluation Complete ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.neural_network import MLPClassifier # The frequentist neural network\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- Data Preparation ---\n",
        "iris = load_iris()\n",
        "X, y_int = iris.data, iris.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_ohe = encoder.fit_transform(y_int.reshape(-1, 1))\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train_int, y_test_int = train_test_split(X_scaled, y_int, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Neural Network Model (Frequentist) ---\n",
        "print(\"--- Training Frequentist Neural Network ---\")\n",
        "\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(10, 5),\n",
        "                          activation='relu',\n",
        "                          solver='adam',\n",
        "                          alpha=0.0001, # L2 regularization (weight decay)\n",
        "                          max_iter=5000, # Max training epochs\n",
        "                          random_state=42,\n",
        "                          verbose=False)\n",
        "\n",
        "# Train the model\n",
        "mlp_model.fit(X_train, y_train_int)\n",
        "\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\n--- Evaluating Frequentist Neural Network Performance ---\")\n",
        "\n",
        "# Predictions\n",
        "train_pred_int = mlp_model.predict(X_train)\n",
        "test_pred_int = mlp_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "train_accuracy = accuracy_score(y_train_int, train_pred_int) * 100\n",
        "test_accuracy = accuracy_score(y_test_int, test_pred_int) * 100\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_test_int, test_pred_int, target_names=iris.target_names))\n",
        "\n",
        "print(\"\\n--- Frequentist Neural Network Performance Evaluation Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816ffafa",
      "metadata": {
        "id": "816ffafa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "web_scrapping",
      "language": "python",
      "name": "web_scrapping"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
