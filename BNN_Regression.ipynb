{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930c3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bb11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- NeuralNetwork Class (for Regression - with mini-batching in mind) ---\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.0001): # Re-evaluating learning rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.num_layers = len(self.layer_sizes) - 1\n",
    "        self.initialize_network()\n",
    "\n",
    "    def initialize_network(self):\n",
    "        for i in range(self.num_layers):\n",
    "            input_dim = self.layer_sizes[i]\n",
    "            output_dim = self.layer_sizes[i+1]\n",
    "            scale_w = np.sqrt(2.0 / (input_dim + output_dim))\n",
    "            self.weights.append(np.random.normal(loc=0, scale=scale_w, size=(input_dim, output_dim)))\n",
    "            self.biases.append(np.random.normal(loc=0, scale=0.01, size=(1, output_dim)))\n",
    "        self.n_params = sum(w.size for w in self.weights) + sum(b.size for b in self.biases)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        X = np.clip(X, -50, 50) # Clip for numerical stability\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.activations = [X]\n",
    "        self.zs = []\n",
    "        for i in range(self.num_layers):\n",
    "            current_input = self.activations[-1]\n",
    "            Z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(Z)\n",
    "            A = self.sigmoid(Z) if i < self.num_layers - 1 else Z\n",
    "            self.activations.append(A)\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        output_predictions = self.activations[-1]\n",
    "        delta = output_predictions - Y\n",
    "\n",
    "        deltas = [delta]\n",
    "        for i in range(self.num_layers - 1, 0, -1):\n",
    "            W_l = self.weights[i]\n",
    "            Z_prev = self.zs[i-1]\n",
    "            delta = (deltas[-1] @ W_l.T) * (self.sigmoid(Z_prev) * (1 - self.sigmoid(Z_prev)))\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            weights_delta = np.dot(self.activations[i].T, deltas[i])\n",
    "            biases_delta = np.sum(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if not np.all(np.isfinite(weights_delta)): weights_delta = np.zeros_like(weights_delta)\n",
    "            if not np.all(np.isfinite(biases_delta)): biases_delta = np.zeros_like(biases_delta)\n",
    "\n",
    "            self.weights[i] -= self.learning_rate * weights_delta\n",
    "            self.biases[i] -= self.learning_rate * biases_delta\n",
    "\n",
    "    def encode(self):\n",
    "        all_params = []\n",
    "        for w in self.weights: all_params.append(w.ravel())\n",
    "        for b in self.biases: all_params.append(b.ravel())\n",
    "        return np.concatenate(all_params)\n",
    "\n",
    "    def decode(self, theta):\n",
    "        decoded_weights = []\n",
    "        decoded_biases = []\n",
    "        offset = 0\n",
    "        for i in range(self.num_layers):\n",
    "            input_dim = self.layer_sizes[i]\n",
    "            output_dim = self.layer_sizes[i+1]\n",
    "            weight_size = input_dim * output_dim\n",
    "            decoded_weights.append(theta[offset : offset + weight_size].reshape(input_dim, output_dim))\n",
    "            offset += weight_size\n",
    "            bias_size = output_dim\n",
    "            decoded_biases.append(theta[offset : offset + bias_size].reshape(1, output_dim))\n",
    "            offset += bias_size\n",
    "        self.weights = decoded_weights\n",
    "        self.biases = decoded_biases\n",
    "\n",
    "    def evaluate_proposal(self, theta, X_data):\n",
    "        temp_weights = []\n",
    "        temp_biases = []\n",
    "        offset = 0\n",
    "        for i in range(self.num_layers):\n",
    "            input_dim = self.layer_sizes[i]\n",
    "            output_dim = self.layer_sizes[i+1]\n",
    "            weight_size = input_dim * output_dim\n",
    "            temp_weights.append(theta[offset : offset + weight_size].reshape(input_dim, output_dim))\n",
    "            offset += weight_size\n",
    "            bias_size = output_dim\n",
    "            temp_biases.append(theta[offset : offset + bias_size].reshape(1, output_dim))\n",
    "            offset += bias_size\n",
    "\n",
    "        activations = [X_data]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            Z = np.dot(activations[-1], temp_weights[i]) + temp_biases[i]\n",
    "            activations.append(self.sigmoid(Z))\n",
    "        fx = np.dot(activations[-1], temp_weights[-1]) + temp_biases[-1]\n",
    "        return fx\n",
    "\n",
    "    # Modified langevin_gradient for mini-batching\n",
    "    def langevin_gradient(self, x_data, y_data, theta, depth, batch_size): # Added batch_size\n",
    "        original_weights = [w.copy() for w in self.weights]\n",
    "        original_biases = [b.copy() for b in self.biases]\n",
    "\n",
    "        self.decode(theta)\n",
    "\n",
    "        num_samples = x_data.shape[0]\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        for _ in range(0, depth):\n",
    "            # Mini-batching\n",
    "            np.random.shuffle(indices)\n",
    "            for start_idx in range(0, num_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, num_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                x_batch = x_data[batch_indices]\n",
    "                y_batch = y_data[batch_indices]\n",
    "\n",
    "                self.forward_pass(x_batch)\n",
    "                self.backward_pass(x_batch, y_batch)\n",
    "\n",
    "        theta_updated = self.encode()\n",
    "\n",
    "        self.weights = original_weights\n",
    "        self.biases = original_biases\n",
    "\n",
    "        return theta_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1206ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MCMC Class ---\n",
    "class MCMC:\n",
    "    def __init__(self, n_samples, n_burnin, x_data, y_data, x_test, y_test, layer_sizes, \n",
    "                 noise_variance=0.1, batch_size=32): # Added batch_size to MCMC __init__\n",
    "        self.n_samples = n_samples\n",
    "        self.n_burnin = n_burnin\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        self.step_theta = 5e-7 # Even smaller MCMC step size might be needed, tune carefully!\n",
    "        self.sigma_squared_prior = 10.0\n",
    "        self.noise_variance = noise_variance\n",
    "\n",
    "        self.model = NeuralNetwork(layer_sizes)\n",
    "        self.theta_size = self.model.n_params\n",
    "\n",
    "        self.use_langevin_gradients = True\n",
    "        self.sgd_depth = 1 # Number of mini-batch updates within one Langevin step\n",
    "        self.l_prob = 0.9 # Higher probability of using Langevin gradient\n",
    "        self.batch_size = batch_size # Mini-batch size for Langevin gradients\n",
    "\n",
    "        self.pos_theta = None\n",
    "        self.pred_y = None\n",
    "        self.rmse_data = None\n",
    "        self.r2_data = None\n",
    "        self.test_pred_y = None\n",
    "        self.test_rmse_data = None\n",
    "        self.test_r2_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_metrics(predictions, targets):\n",
    "        if not np.all(np.isfinite(predictions)) or not np.all(np.isfinite(targets)):\n",
    "            return np.inf, -np.inf\n",
    "        rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "        r2 = r2_score(targets, predictions)\n",
    "        return rmse, r2\n",
    "\n",
    "    def likelihood_function(self, theta, test=False):\n",
    "        x_data_eval = self.x_test if test else self.x_data\n",
    "        y_data_eval = self.y_test if test else self.y_data\n",
    "\n",
    "        model_prediction = self.model.evaluate_proposal(theta, x_data_eval)\n",
    "\n",
    "        if not np.all(np.isfinite(model_prediction)):\n",
    "            return -np.inf, model_prediction, np.inf, -np.inf\n",
    "\n",
    "        N = y_data_eval.shape[0]\n",
    "        sum_sq_error = np.sum(np.square(y_data_eval - model_prediction))\n",
    "\n",
    "        if self.noise_variance <= 1e-9:\n",
    "            return -np.inf, model_prediction, np.inf, -np.inf\n",
    "\n",
    "        log_likelihood = -N/2.0 * np.log(2 * np.pi * self.noise_variance) - (1/(2 * self.noise_variance)) * sum_sq_error\n",
    "        \n",
    "        rmse, r2 = self.evaluate_metrics(model_prediction, y_data_eval)\n",
    "\n",
    "        return log_likelihood, model_prediction, rmse, r2\n",
    "\n",
    "    def prior(self, sigma_squared_prior, theta):\n",
    "        if sigma_squared_prior <= 1e-9:\n",
    "            return -np.inf\n",
    "        part1 = -self.model.n_params / 2.0 * np.log(sigma_squared_prior)\n",
    "        part2 = 1 / (2.0 * sigma_squared_prior) * (np.sum(np.square(theta)))\n",
    "        return part1 - part2\n",
    "\n",
    "    def MCMC_sampler(self):\n",
    "        output_dim = self.model.layer_sizes[-1]\n",
    "        pos_theta = np.zeros((self.n_samples, self.theta_size))\n",
    "        pred_y = np.zeros((self.n_samples, self.x_data.shape[0], output_dim))\n",
    "        test_pred_y = np.zeros((self.n_samples, self.x_test.shape[0], output_dim))\n",
    "        rmse_data = np.zeros(self.n_samples)\n",
    "        r2_data = np.zeros(self.n_samples)\n",
    "        test_rmse_data = np.zeros(self.n_samples)\n",
    "        test_r2_data = np.zeros(self.n_samples)\n",
    "\n",
    "        theta = np.random.randn(self.theta_size) * 0.01 # Smaller initial scale\n",
    "\n",
    "        prior_val = self.prior(self.sigma_squared_prior, theta)\n",
    "        (likelihood, initial_train_pred, initial_train_rmse, initial_train_r2) = self.likelihood_function(theta, test=False)\n",
    "        (test_likelihood, initial_test_pred, initial_test_rmse, initial_test_r2) = self.likelihood_function(theta, test=True)\n",
    "\n",
    "        if not np.isfinite(likelihood) or not np.isfinite(prior_val):\n",
    "            print(\"Warning: Initial likelihood or prior is non-finite. Re-initializing theta to a smaller scale.\")\n",
    "            theta = np.random.randn(self.theta_size) * 0.001\n",
    "            prior_val = self.prior(self.sigma_squared_prior, theta)\n",
    "            (likelihood, initial_train_pred, initial_train_rmse, initial_train_r2) = self.likelihood_function(theta, test=False)\n",
    "            (test_likelihood, initial_test_pred, initial_test_rmse, initial_test_r2) = self.likelihood_function(theta, test=True)\n",
    "            if not np.isfinite(likelihood) or not np.isfinite(prior_val):\n",
    "                 print(\"Critical: Initial likelihood/prior still non-finite after re-init. MCMC chain may struggle.\")\n",
    "\n",
    "        pos_theta[0, :] = theta\n",
    "        pred_y[0, :, :] = initial_train_pred\n",
    "        rmse_data[0] = initial_train_rmse\n",
    "        r2_data[0] = initial_train_r2\n",
    "        test_pred_y[0, :, :] = initial_test_pred\n",
    "        test_rmse_data[0] = initial_test_rmse\n",
    "        test_r2_data[0] = initial_test_r2\n",
    "\n",
    "        n_accepted_samples = 0\n",
    "\n",
    "        print(f\"MCMC Chain started for {self.n_samples} samples with batch_size={self.batch_size}...\")\n",
    "        print(f\"Initial Train RMSE: {initial_train_rmse:.4f}, R2: {initial_train_r2:.4f}\")\n",
    "        print(f\"Initial Test RMSE: {initial_test_rmse:.4f}, R2: {initial_test_r2:.4f}\")\n",
    "\n",
    "        for ii in range(1, self.n_samples):\n",
    "            theta_current = pos_theta[ii - 1, :]\n",
    "            prior_current = prior_val\n",
    "            likelihood_current = likelihood\n",
    "\n",
    "            lx = np.random.uniform(0, 1)\n",
    "            if lx < self.l_prob and self.use_langevin_gradients:\n",
    "                # Pass batch_size to langevin_gradient\n",
    "                theta_gd = self.model.langevin_gradient(self.x_data, self.y_data, theta_current, self.sgd_depth, self.batch_size)\n",
    "                theta_proposal = np.random.normal(theta_gd, self.step_theta, self.theta_size)\n",
    "            else:\n",
    "                theta_proposal = np.random.normal(theta_current, self.step_theta, self.theta_size)\n",
    "\n",
    "            prior_proposal = self.prior(self.sigma_squared_prior, theta_proposal)\n",
    "            (likelihood_proposal, current_train_pred, current_train_rmse, current_train_r2) = self.likelihood_function(theta_proposal, test=False)\n",
    "            (test_likelihood_proposal, current_test_pred, current_test_rmse, current_test_r2) = self.likelihood_function(theta_proposal, test=True)\n",
    "\n",
    "            if not np.isfinite(likelihood_proposal) or not np.isfinite(prior_proposal):\n",
    "                mh_prob = 0.0\n",
    "            else:\n",
    "                diff_likelihood = likelihood_proposal - likelihood_current\n",
    "                diff_prior = prior_proposal - prior_current\n",
    "\n",
    "                diff_prop = 0.0\n",
    "                if lx < self.l_prob and self.use_langevin_gradients:\n",
    "                    # Pass batch_size to langevin_gradient for reverse step\n",
    "                    theta_gd_reverse = self.model.langevin_gradient(self.x_data, self.y_data, theta_proposal, self.sgd_depth, self.batch_size)\n",
    "                    log_q_prop_given_curr = -0.5 * np.sum(np.square(theta_proposal - theta_gd)) / (self.step_theta**2)\n",
    "                    log_q_curr_given_prop = -0.5 * np.sum(np.square(theta_current - theta_gd_reverse)) / (self.step_theta**2)\n",
    "                    diff_prop = log_q_curr_given_prop - log_q_prop_given_curr\n",
    "                    if not np.isfinite(diff_prop): mh_prob = 0.0\n",
    "\n",
    "                if np.isfinite(diff_likelihood) and np.isfinite(diff_prior) and np.isfinite(diff_prop):\n",
    "                    mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
    "                else:\n",
    "                    mh_prob = 0.0\n",
    "\n",
    "            u = np.random.uniform(0, 1)\n",
    "\n",
    "            if u < mh_prob:\n",
    "                pos_theta[ii, :] = theta_proposal\n",
    "                likelihood = likelihood_proposal\n",
    "                prior_val = prior_proposal\n",
    "                pred_y[ii, :, :] = current_train_pred\n",
    "                rmse_data[ii] = current_train_rmse\n",
    "                r2_data[ii] = current_train_r2\n",
    "                test_pred_y[ii, :, :] = current_test_pred\n",
    "                test_rmse_data[ii] = current_test_rmse\n",
    "                test_r2_data[ii] = current_test_r2\n",
    "                n_accepted_samples += 1\n",
    "            else:\n",
    "                pos_theta[ii, :] = pos_theta[ii - 1, :]\n",
    "                pred_y[ii, :, :] = pred_y[ii - 1, :, :]\n",
    "                rmse_data[ii] = rmse_data[ii - 1]\n",
    "                r2_data[ii] = r2_data[ii - 1]\n",
    "                test_pred_y[ii, :, :] = test_pred_y[ii - 1, :, :]\n",
    "                test_rmse_data[ii] = test_rmse_data[ii - 1]\n",
    "                test_r2_data[ii] = test_r2_data[ii - 1]\n",
    "\n",
    "            if (ii + 1) % 500 == 0:\n",
    "                acceptance_rate = (n_accepted_samples / (ii + 1)) * 100\n",
    "                print(f\"Sample {ii+1}/{self.n_samples} | Accept Rate: {acceptance_rate:.2f}% | \"\n",
    "                      f\"Train RMSE: {rmse_data[ii]:.4f}, R2: {r2_data[ii]:.4f} | \"\n",
    "                      f\"Test RMSE: {test_rmse_data[ii]:.4f}, R2: {test_r2_data[ii]:.4f}\")\n",
    "\n",
    "        print(\"MCMC sampling complete. Applying burn-in.\")\n",
    "        \n",
    "        self.pos_theta = pos_theta[self.n_burnin:, :]\n",
    "        self.pred_y = pred_y[self.n_burnin:, :, :]\n",
    "        self.rmse_data = rmse_data[self.n_burnin:]\n",
    "        self.r2_data = r2_data[self.n_burnin:]\n",
    "        self.test_pred_y = test_pred_y[self.n_burnin:, :, :]\n",
    "        self.test_rmse_data = test_rmse_data[self.n_burnin:]\n",
    "        self.test_r2_data = test_r2_data[self.n_burnin:]\n",
    "\n",
    "        results_df = pd.DataFrame(self.pos_theta, columns=[f\"theta_{i}\" for i in range(self.theta_size)])\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9441728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation and Execution for Regression ---\n",
    "\n",
    "def load_and_preprocess_regression_data(dataset_loader, test_size=0.2, random_state=42):\n",
    "    data = dataset_loader()\n",
    "    X, y = data.data, data.target.reshape(-1, 1)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y = scaler_y.fit_transform(y) # Scale target variable too!\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e318bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BNN for Regression on California Housing Dataset (Faster NumPy Version)\n",
      "==================================================\n",
      "Network Architecture for Regression: [8, 50, 25, 1]\n"
     ]
    }
   ],
   "source": [
    "# --- Run Regression Example ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing BNN for Regression on California Housing Dataset (Faster NumPy Version)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg, scaler_X_reg, scaler_y_reg = load_and_preprocess_regression_data(fetch_california_housing)\n",
    "\n",
    "input_dim_reg = X_train_reg.shape[1]\n",
    "output_dim_reg = y_train_reg.shape[1]\n",
    "hidden_layer_nodes_reg = [50, 25]\n",
    "layer_sizes_reg = [input_dim_reg] + hidden_layer_nodes_reg + [output_dim_reg]\n",
    "\n",
    "print(f\"Network Architecture for Regression: {layer_sizes_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414333ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MCMC sampling for Regression dataset...\n",
      "MCMC Chain started for 50000 samples with batch_size=128...\n",
      "Initial Train RMSE: 1.0020, R2: -0.0001\n",
      "Initial Test RMSE: 0.9920, R2: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_32956\\2810185149.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_32956\\2810185149.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_32956\\2810185149.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_32956\\2810185149.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_32956\\2810185149.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  mh_prob = np.min([1.0, np.exp(diff_likelihood + diff_prior + diff_prop)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 500/50000 | Accept Rate: 7.00% | Train RMSE: 0.5839, R2: 0.6604 | Test RMSE: 0.5968, R2: 0.6381\n",
      "Sample 1000/50000 | Accept Rate: 9.60% | Train RMSE: 0.5427, R2: 0.7066 | Test RMSE: 0.5525, R2: 0.6898\n",
      "Sample 1500/50000 | Accept Rate: 9.93% | Train RMSE: 0.5235, R2: 0.7271 | Test RMSE: 0.5326, R2: 0.7118\n",
      "Sample 2000/50000 | Accept Rate: 9.30% | Train RMSE: 0.5144, R2: 0.7364 | Test RMSE: 0.5238, R2: 0.7212\n",
      "Sample 2500/50000 | Accept Rate: 8.40% | Train RMSE: 0.5092, R2: 0.7417 | Test RMSE: 0.5191, R2: 0.7262\n",
      "Sample 3000/50000 | Accept Rate: 7.80% | Train RMSE: 0.5006, R2: 0.7504 | Test RMSE: 0.5106, R2: 0.7351\n",
      "Sample 3500/50000 | Accept Rate: 7.49% | Train RMSE: 0.4896, R2: 0.7613 | Test RMSE: 0.4992, R2: 0.7468\n",
      "Sample 4000/50000 | Accept Rate: 7.35% | Train RMSE: 0.4827, R2: 0.7679 | Test RMSE: 0.4923, R2: 0.7537\n",
      "Sample 4500/50000 | Accept Rate: 7.44% | Train RMSE: 0.4782, R2: 0.7723 | Test RMSE: 0.4876, R2: 0.7584\n"
     ]
    }
   ],
   "source": [
    "# MCMC parameters (tuned for faster NumPy execution and more stability)\n",
    "n_samples_reg = 50000 # Keep samples high for good posterior estimation\n",
    "n_burnin_reg = 10000\n",
    "noise_variance_reg = 0.1\n",
    "batch_size_reg = 128 # Mini-batch size for Langevin gradients (CRITICAL for speed)\n",
    "\n",
    "print(\"Starting MCMC sampling for Regression dataset...\")\n",
    "bnn_mcmc_reg = MCMC(n_samples_reg, n_burnin_reg, \n",
    "                    X_train_reg, y_train_reg, \n",
    "                    X_test_reg, y_test_reg, \n",
    "                    layer_sizes_reg, \n",
    "                    noise_variance=noise_variance_reg,\n",
    "                    batch_size=batch_size_reg)\n",
    "posterior_samples_df_reg = bnn_mcmc_reg.MCMC_sampler() \n",
    "print(\"MCMC sampling for Regression dataset finished.\")\n",
    "\n",
    "print(\"\\nShape of posterior samples (after burn-in):\", posterior_samples_df_reg.shape)\n",
    "print(\"First 5 posterior samples:\\n\", posterior_samples_df_reg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa494da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Performance for Regression Dataset ---\n",
    "print(\"\\n--- Final Performance Evaluation for Regression Dataset ---\")\n",
    "    \n",
    "mean_train_predictions_reg_scaled = np.mean(bnn_mcmc_reg.pred_y, axis=0)\n",
    "mean_test_predictions_reg_scaled = np.mean(bnn_mcmc_reg.test_pred_y, axis=0)\n",
    "\n",
    "final_train_predictions_reg = scaler_y_reg.inverse_transform(mean_train_predictions_reg_scaled)\n",
    "final_test_predictions_reg = scaler_y_reg.inverse_transform(mean_test_predictions_reg_scaled)\n",
    "\n",
    "original_y_train_reg = scaler_y_reg.inverse_transform(y_train_reg)\n",
    "original_y_test_reg = scaler_y_reg.inverse_transform(y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727603ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_rmse, final_train_r2 = mean_squared_error(original_y_train_reg, final_train_predictions_reg), r2_score(original_y_train_reg, final_train_predictions_reg)\n",
    "final_test_rmse, final_test_r2 = mean_squared_error(original_y_test_reg, final_test_predictions_reg), r2_score(original_y_test_reg, final_test_predictions_reg)\n",
    "\n",
    "print(f\"\\nFinal Train RMSE (from posterior predictive mean): {final_train_rmse:.4f}\")\n",
    "print(f\"Final Train R2 (from posterior predictive mean): {final_train_r2:.4f}\")\n",
    "print(f\"Final Test RMSE (from posterior predictive mean): {final_test_rmse:.4f}\")\n",
    "print(f\"Final Test R2 (from posterior predictive mean): {final_test_r2:.4f}\")\n",
    "\n",
    "avg_sampled_train_rmse = np.mean(bnn_mcmc_reg.rmse_data[np.isfinite(bnn_mcmc_reg.rmse_data)])\n",
    "avg_sampled_train_r2 = np.mean(bnn_mcmc_reg.r2_data[np.isfinite(bnn_mcmc_reg.r2_data)])\n",
    "avg_sampled_test_rmse = np.mean(bnn_mcmc_reg.test_rmse_data[np.isfinite(bnn_mcmc_reg.test_rmse_data)])\n",
    "avg_sampled_test_r2 = np.mean(bnn_mcmc_reg.test_r2_data[np.isfinite(bnn_mcmc_reg.test_r2_data)])\n",
    "\n",
    "print(f\"\\nAverage Train RMSE (across accepted finite samples): {avg_sampled_train_rmse:.4f}\")\n",
    "print(f\"Average Train R2 (across accepted finite samples): {avg_sampled_train_r2:.4f}\")\n",
    "print(f\"Average Test RMSE (across accepted finite samples): {avg_sampled_test_rmse:.4f}\")\n",
    "print(f\"Average Test R2 (across accepted finite samples): {avg_sampled_test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"End of Faster NumPy Regression BNN Test\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf25fa3",
   "metadata": {},
   "source": [
    "## SKLEARN RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Data Loading and Preprocessing \n",
    "def load_and_preprocess_regression_data(dataset_loader, test_size=0.2, random_state=42):\n",
    "    data = dataset_loader()\n",
    "    X, y = data.data, data.target.reshape(-1, 1)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y = scaler_y.fit_transform(y) \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing California Housing Data ---\n",
      "Training features shape: (16512, 8)\n",
      "Training target shape: (16512, 1)\n",
      "Test features shape: (4128, 8)\n",
      "Test target shape: (4128, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare California Housing Data ---\n",
    "print(\"--- Preparing California Housing Data ---\")\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg, scaler_X_reg, scaler_y_reg = load_and_preprocess_regression_data(fetch_california_housing)\n",
    "\n",
    "print(f\"Training features shape: {X_train_reg.shape}\")\n",
    "print(f\"Training target shape: {y_train_reg.shape}\")\n",
    "print(f\"Test features shape: {X_test_reg.shape}\")\n",
    "print(f\"Test target shape: {y_test_reg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db601f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Linear Regression Model ---\n"
     ]
    }
   ],
   "source": [
    "# --- Train a Simple Linear Regression Model ---\n",
    "print(\"\\n--- Training Linear Regression Model ---\")\n",
    "\n",
    "# Create the model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model using the scaled training data\n",
    "linear_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# --- Make Predictions ---\n",
    "# Predict on the scaled training data\n",
    "train_predictions_scaled = linear_model.predict(X_train_reg)\n",
    "# Predict on the scaled test data\n",
    "test_predictions_scaled = linear_model.predict(X_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd310cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Linear Regression Performance ---\n",
      "Train RMSE: 0.5179\n",
      "Train R2 Score: 0.6126\n",
      "Test RMSE: 0.5559\n",
      "Test R2 Score: 0.5758\n",
      "\n",
      "--- Linear Regression Performance Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Performance (Inverse transform to original scale for metrics) ---\n",
    "print(\"\\n--- Evaluating Linear Regression Performance ---\")\n",
    "\n",
    "# Inverse transform predictions and true values to original scale\n",
    "final_train_predictions = scaler_y_reg.inverse_transform(train_predictions_scaled)\n",
    "final_test_predictions = scaler_y_reg.inverse_transform(test_predictions_scaled)\n",
    "\n",
    "original_y_train = scaler_y_reg.inverse_transform(y_train_reg)\n",
    "original_y_test = scaler_y_reg.inverse_transform(y_test_reg)\n",
    "\n",
    "# Calculate RMSE and R2 for training set\n",
    "train_rmse = mean_squared_error(original_y_train, final_train_predictions)\n",
    "train_r2 = r2_score(original_y_train, final_train_predictions)\n",
    "\n",
    "# Calculate RMSE and R2 for test set\n",
    "test_rmse = mean_squared_error(original_y_test, final_test_predictions)\n",
    "test_r2 = r2_score(original_y_test, final_test_predictions)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Train R2 Score: {train_r2:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R2 Score: {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- Linear Regression Performance Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b0e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scrapping",
   "language": "python",
   "name": "web_scrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
